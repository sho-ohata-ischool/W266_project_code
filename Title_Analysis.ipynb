{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shooh_000\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import codecs\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import gensim\n",
    "import nltk\n",
    "import random\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy import sparse\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from extract_entities import extract_entity_names, return_entity_list ## helper function for entity extraction\n",
    "from gensim.models import KeyedVectors\n",
    "from pyemd import emd ## Word mover's distance\n",
    "\n",
    "## Use word2vec embeddings\n",
    "## Code source: http://nbviewer.jupyter.org/github/vene/vene.github.io/blob/pelican/content/blog/word-movers-distance-in-python.ipynb\n",
    "if not os.path.exists(\"data/embed.dat\"):\n",
    "    print(\"Caching word embeddings in memmapped format...\")  \n",
    "    wv = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        \"data/GoogleNews-vectors-negative300.bin.gz\",\n",
    "        binary=True)\n",
    "    wv.init_sims(replace=True) # To load L2 normalized vectors in wv.syn0norm from wv.syn0\n",
    "    fp = np.memmap(\"data/embed.dat\", dtype=np.double, mode='w+', shape=wv.syn0.shape)\n",
    "    fp[:] = wv.syn0[:]\n",
    "    with open(\"data/embed.vocab\", \"w\", encoding='utf-8') as f:\n",
    "        for _, w in sorted((voc.index, word) for word, voc in wv.vocab.items()):\n",
    "            print(w, file=f)\n",
    "    del fp, wv\n",
    "\n",
    "W = np.memmap(\"data/embed.dat\", dtype=np.double, mode=\"r\", shape=(3000000, 300))\n",
    "with open(\"data/embed.vocab\", encoding='utf-8') as f:\n",
    "    vocab_list = map(str.strip, f.readlines())\n",
    "    \n",
    "vocab_dict = {w: k for k, w in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Loading title data from The Onion, Borowitz Report and New York Times. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Borowitz Report articles: 356\n",
      "Number of The Onion articles: 3279\n",
      "Number of New York Times articles: 120869\n"
     ]
    }
   ],
   "source": [
    "def convert_num_in_title(ER_dict):\n",
    "    for key in list(ER_dict.keys()):\n",
    "        ER_dict[key]['title'] = re.sub('\\d', '#', ER_dict[key]['title']) ## Numbers represented as # in word2vec\n",
    "        ER_dict[key]['datetime'] = datetime.strptime(ER_dict[key]['timestamp'][0:10], '%Y-%m-%d')\n",
    "    return ER_dict\n",
    "\n",
    "with open('borowitz_report.json', 'r') as f:\n",
    "    bw = json.load(f)\n",
    "\n",
    "with open('the_onion.json', 'r') as f:\n",
    "    onion = json.load(f)    \n",
    "\n",
    "with open('NY_times.json', 'r') as f:\n",
    "    ny_times = json.load(f)\n",
    "    \n",
    "bw, onion, ny_times = convert_num_in_title(bw), convert_num_in_title(onion), convert_num_in_title(ny_times)\n",
    "    \n",
    "all_combined = dict(list(bw.items()) + list(onion.items()) + list(ny_times.items()))\n",
    "\n",
    "print('Number of Borowitz Report articles: %d' %len(bw))\n",
    "print('Number of The Onion articles: %d' %len(onion))\n",
    "print('Number of New York Times articles: %d' %len(ny_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique titles from all: 107929\n",
      "Number of unique words from all titles: 42204\n",
      "Number of words found in word2vec: 39778\n",
      "Barack_Obama: True\n",
      "New_York: True\n",
      "Example Borowitz Report Title: Sentiment Building to Deport Nation's Billionaires\n"
     ]
    }
   ],
   "source": [
    "## Just some quick stats of the titles\n",
    "\n",
    "all_titles = []\n",
    "for x in all_combined.values():\n",
    "    all_titles.append(x['title'])\n",
    "\n",
    "all_titles = list(set(all_titles))\n",
    "\n",
    "print ('Unique titles from all: %d' %len(all_titles))\n",
    "\n",
    "vect = CountVectorizer(stop_words=\"english\", lowercase=False).fit(all_titles)\n",
    "\n",
    "print ('Number of unique words from all titles: %d' %len(vect.get_feature_names()))\n",
    "print ('Number of words found in word2vec: %d' %len([w for w in vect.get_feature_names() if w in vocab_dict]))\n",
    "\n",
    "## Entities with 2 or more words will need to be identified and modified so they can be mapped correctly in the word embeddings\n",
    "## There are also 2250 words that appear that are not in the word embeddings. \n",
    "print ('Barack_Obama: %s' %str('Barack_Obama' in vocab_dict)) \n",
    "print ('New_York: %s' %str('New_York' in vocab_dict)) \n",
    "print ('Example Borowitz Report Title: %s' %all_combined['51685819']['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Borowitz Report title: Rand Paul No Longer Most Embarrassing Thing About Kentucky\n",
      "Example Borowitz Report title after modification: Rand_Paul No longer most embarrassing thing about Kentucky\n"
     ]
    }
   ],
   "source": [
    "## This section of the code will modify the existing titles so they are a bit more \"sentence\" like. Non-entity words like\n",
    "## play, basketball will be converted to lower-case while entites like New York will be converted to New_York.\n",
    "\n",
    "## Function to reformat title so non-entity words are made lower case and entity words remain upper case.\n",
    "## Function also converts words like New York to New_York\n",
    "def reformat_title(entities, entities_list_len2, ER_dict): ## pass entities, entites that are \n",
    "    for key in ER_dict.keys():\n",
    "        title = ER_dict[key]['title']\n",
    "        reformatted = ''\n",
    "        for entity in entities_list_len2:\n",
    "            if entity in title:\n",
    "                entity_ = re.sub(' ', '_', entity)\n",
    "                title = re.sub(entity, entity_, title)\n",
    "\n",
    "        for index, string in enumerate(title.split()):\n",
    "            if index != 0 and string not in entities and '_' not in string: ## make word lower case and add _\n",
    "                reformatted = reformatted + ' ' + string.lower()\n",
    "\n",
    "            elif index == 0: ## First word remains as is\n",
    "                reformatted = string\n",
    "                \n",
    "            else:\n",
    "                reformatted = reformatted + ' ' + string ## add word as is\n",
    "              \n",
    "        ER_dict[key]['reformatted_title'] = reformatted\n",
    "    return ER_dict\n",
    "\n",
    "with open('theonion_fulltext.json',encoding='utf-8') as f:\n",
    "    full_text_json = json.load(f)\n",
    "    \n",
    "onion_texts = []\n",
    "for key in full_text_json.keys():\n",
    "    try:\n",
    "        onion_texts.append(full_text_json[key]['full_text'])\n",
    "    except:\n",
    "        next\n",
    "\n",
    "onion_texts = list(set(onion_texts))\n",
    "entities, entities_list_len2 = return_entity_list(onion_texts) ## This is a helper script to extract named entities\n",
    "onion = reformat_title(entities, entities_list_len2, onion)\n",
    "del onion_texts\n",
    "\n",
    "## Borowitz Report\n",
    "with open('borowitz_fulltext.json',encoding='utf-8') as f:\n",
    "    full_text_json = json.load(f)\n",
    "    \n",
    "borowitz_texts = []\n",
    "for key in full_text_json.keys():\n",
    "    try:\n",
    "        borowitz_texts.append(full_text_json[key]['full_text'])\n",
    "    except:\n",
    "        next\n",
    "\n",
    "borowitz_texts = list(set(borowitz_texts))\n",
    "entities_, entities_list_len2_ = return_entity_list(borowitz_texts) ## This is a helper script to extract entities\n",
    "entities.extend(entities_)\n",
    "entities_list_len2.extend(entities_list_len2_)\n",
    "bw = reformat_title(entities, entities_list_len2, bw)\n",
    "del borowitz_texts, entities_, entities_list_len2_\n",
    "\n",
    "## NY Times\n",
    "with open('nytimes_fulltext.json',encoding='utf-8') as f:\n",
    "    full_text_json = json.load(f)\n",
    "    \n",
    "nytimes_texts = []\n",
    "for key in full_text_json.keys():\n",
    "    try:\n",
    "        nytimes_texts.append(full_text_json[key]['full_text'])\n",
    "    except:\n",
    "        next\n",
    "\n",
    "nytimes_texts = list(set(nytimes_texts))\n",
    "entities_, entities_list_len2_ = return_entity_list(nytimes_texts) ## This is a helper script to extract entities\n",
    "ny_times = reformat_title(entities, entities_list_len2, ny_times)\n",
    "entities.extend(entities_)\n",
    "entities_list_len2.extend(entities_list_len2_)\n",
    "del nytimes_texts, entities_, entities_list_len2_\n",
    "\n",
    "## Re-combine all dict \n",
    "all_combined = dict(list(bw.items()) + list(onion.items()) + list(ny_times.items()))\n",
    "\n",
    "## Also reformat titles in related_articles\n",
    "for key in all_combined.keys():\n",
    "    if 'related_articles' in all_combined[key].keys():\n",
    "        all_combined[key]['related_articles'] = reformat_title(entities, entities_list_len2, all_combined[key]['related_articles'])\n",
    "\n",
    "print ('Example Borowitz Report title: %s' %all_combined['61852980']['title'])\n",
    "print ('Example Borowitz Report title after modification: %s' %all_combined['61852980']['reformatted_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Helper function that will generate a list of candidate articles, i.e. all related articles listed in Event Registry\n",
    "## as well as all NY Times articles that are within 2 days of the satirical article.\n",
    "## The output is a tuple so it can be mapped back to the original unedited article title.\n",
    "def candidate_articles_list(ER_value): \n",
    "    \n",
    "    candidate_articles = []\n",
    "    date_time = ER_value['datetime']\n",
    "    if 'related_articles' in ER_value.keys():\n",
    "        for key in ER_value['related_articles'].keys():\n",
    "            title = ''\n",
    "            for index, word in enumerate(ER_value['related_articles'][key]['title'].split()):\n",
    "                word = re.sub('\\.', '', word)\n",
    "                if word in vocab_dict and index != 0:\n",
    "                    title = title + ' ' + word\n",
    "                elif word in vocab_dict and index == 0:\n",
    "                    title = word\n",
    "                else:\n",
    "                    next\n",
    "            candidate_articles.append((key, title))\n",
    "\n",
    "    for key, value in ny_times.items():\n",
    "        if value['datetime'] <= date_time and value['datetime'] >= date_time + timedelta(days=-2):\n",
    "            title = ''\n",
    "            for index, word in enumerate(value['reformatted_title'].split()):\n",
    "                word = re.sub('\\.', '', word)\n",
    "                if word in vocab_dict and index != 0:\n",
    "                    title = title + ' ' + word\n",
    "                elif word in vocab_dict and index == 0:\n",
    "                    title = word\n",
    "                else:\n",
    "                    next\n",
    "            candidate_articles.append((key,title))\n",
    "    return list(set(candidate_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_titles = []\n",
    "for key, values in all_combined.items():\n",
    "    if values['reformatted_title'] not in all_titles:\n",
    "        all_titles.append(values['reformatted_title'])\n",
    "        \n",
    "vect_total = CountVectorizer(stop_words=\"english\", lowercase=False).fit(all_titles)\n",
    "common = [word for word in vect_total.get_feature_names() if word in vocab_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Satire article: Jeb Bush Resigns as George W. Bush's Brother \n",
      "\n",
      "Candidate articles:\n",
      "Jeb Bush Resigns From Board Seats, Possibly Edging Closer to Presidential Run\n",
      "No. ## Oklahoma Shuts Down George Mason, ##-##\n",
      "Jeb Bush Won’t Attend Immigration Critic’s Event in Iowa\n",
      "Jeb Bush resigns from all boards\n",
      "Jeb Bush resigns from board memberships\n"
     ]
    }
   ],
   "source": [
    "## Given input of the satirical article to be mapped, along with list of candidate articles this function will \n",
    "## print out the top 5 candidate articles according to cosine similarity based on tf-idf scores.\n",
    "\n",
    "def mapped_articles_tfidf(key_related, satire_article, candidate_articles_tuples):\n",
    "    candidate_articles = list(set([x[1] for x in candidate_articles_tuples]))\n",
    "    vectorizer = TfidfVectorizer(vocabulary=common, stop_words=\"english\", lowercase=False).fit([satire_article] + candidate_articles)\n",
    "    cosine_similarities = linear_kernel(vectorizer.transform([satire_article]), \n",
    "                                        sparse.csr_matrix(vectorizer.transform(candidate_articles))).flatten()\n",
    "    related_docs_indices = cosine_similarities.argsort()[:-6:-1]\n",
    "    print ('Satire article: %s \\n' %all_combined[key_related]['title'])\n",
    "    print ('Candidate articles:')\n",
    "    for index in related_docs_indices:\n",
    "        key = [x[0] for x in candidate_articles_tuples if x[1] == candidate_articles[index]][0]\n",
    "        try:\n",
    "            print (all_combined[key]['title'])\n",
    "        except:\n",
    "            print(all_combined[key_related]['related_articles'][key]['title'])\n",
    "\n",
    "satire_article = all_combined['25518311']['reformatted_title']\n",
    "candidate_articles = candidate_articles_list(all_combined['25518311'])\n",
    "mapped_articles_tfidf('25518311',satire_article, candidate_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Satire article: Jeb Bush Resigns as George W. Bush's Brother \n",
      "\n",
      "Candidate articles:\n",
      "Edith Pearlman’s ‘Honeydew’: score: 0.83\n",
      "A Simple Gift: score: 1.85\n",
      "Soy on the Lower East Side: score: 0.87\n",
      "The Rise of Evgeny Lebedev: score: 1.83\n",
      "Can Writers Still ‘Make It New’?: score: 1.84\n"
     ]
    }
   ],
   "source": [
    "## Given input of the satirical article to be mapped, along with list of candidate articles this function will \n",
    "## print out the top 5 candidate articles according to Word Mover's distance.\n",
    "def mapped_articles_wmd(key_related, satire_article, candidate_articles_tuples):\n",
    "    candidate_articles = list(set([x[1] for x in candidate_articles_tuples]))\n",
    "    vect_wmd = CountVectorizer(stop_words=\"english\", lowercase=False).fit([satire_article] + candidate_articles)\n",
    "    common_wmd = [word for word in vect_wmd.get_feature_names()]\n",
    "    W_common_wmd = W[[vocab_dict[w] for w in common_wmd]]\n",
    "    D_ = euclidean_distances(W_common_wmd)\n",
    "    v_1 = vect_wmd.transform([satire_article]).toarray().ravel().astype(np.double)\n",
    "    v_1 /= v_1.sum()\n",
    "    D_ = D_.astype(np.double)\n",
    "    D_ /= D_.max()  # just for comparison purposes\n",
    "    top_5_scores = {}\n",
    "\n",
    "    for index, title in enumerate(candidate_articles):\n",
    "        v_2 = vect_wmd.transform([candidate_articles[index]]).toarray().ravel().astype(np.double)\n",
    "        emd_score = emd(v_1, v_2, D_)\n",
    "        if len(top_5_scores) < 5:\n",
    "            top_5_scores[title] = emd_score\n",
    "        else:\n",
    "            if emd_score < max(top_5_scores.values()):\n",
    "                top_5_scores[title] = emd_score\n",
    "                for key in list(top_5_scores.keys()):\n",
    "                    if top_5_scores[key] == max(top_5_scores.values()):\n",
    "                        del top_5_scores[key]\n",
    "                \n",
    "    print ('Satire article: %s \\n' %all_combined[key_related]['title'])\n",
    "    print ('Candidate articles:')\n",
    "    for title, values in top_5_scores.items():\n",
    "        key = [x[0] for x in candidate_articles_tuples if x[1] == title][0]\n",
    "        try:\n",
    "            print ('%s: score: %.2f' %(all_combined[key]['title'], values))\n",
    "        except:\n",
    "            print('%s: score: %.2f' %(all_combined[key_related]['related_articles'][key]['title'], values))  \n",
    "                \n",
    "satire_article = all_combined['25518311']['title']\n",
    "candidate_articles = candidate_articles_list(all_combined['25518311'])\n",
    "mapped_articles_wmd('25518311', satire_article, candidate_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf:\n",
      "Satire article: Trump Unveils Sprawling New Presidential Retreat Where He Can Escape From Stresses Of Mar-A-Lago \n",
      "\n",
      "Candidate articles:\n",
      "GAO Agrees To Review Costs Of Trump's Trips To Mar-A-Lago\n",
      "Joe Girardi Trots Out a New Look for the Yankees\n",
      "He Turned His Home Into a Reality Television Show\n",
      "Where Everyone 'Knows Hockey': Tiny Clarkson Stands Tall Again\n",
      "Mar-A-Lago Act: Bill To Force Trump To Publish Visitors As He Skips Trip No. 6\n",
      "\n",
      "Word Mover's:\n",
      "Satire article: Trump Unveils Sprawling New Presidential Retreat Where He Can Escape From Stresses Of Mar-A-Lago \n",
      "\n",
      "Candidate articles:\n",
      "Just Bulbs: Still Burning Bright on the Upper East Side: score: 2.81\n",
      "#### Dilemma for Republicans: Which Way Now on Obamacare?: score: 2.82\n",
      "How Well Do You Sleep?: score: 0.80\n",
      "What's Going On in This Picture? | March ##, ####: score: 2.79\n",
      "'Billions' Season #, Episode # Recap: Wendy's Back: score: 2.80\n"
     ]
    }
   ],
   "source": [
    "## Onion article with related articles\n",
    "satire_article = all_combined['26464958']['title']\n",
    "candidate_articles = candidate_articles_list(all_combined['26464958'])\n",
    "print('tf-idf:')\n",
    "mapped_articles_tfidf('26464958',satire_article, candidate_articles)\n",
    "print(\"\\nWord Mover's:\")\n",
    "mapped_articles_wmd('26464958', satire_article, candidate_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf:\n",
      "Satire article: Secret Service Adds Emotional Protection Division To Safeguard Trump’s Psyche \n",
      "\n",
      "Candidate articles:\n",
      "Trump Seems to Side With Russia in Comments on Ukraine\n",
      "How Trump Chose His Supreme Court Nominee\n",
      "How Attorneys General Became Democrats' Bulwark Against Trump\n",
      "A Quiet Giant of Investing Weighs In on Trump\n",
      "Palestinians Fear Being Sidelined by Trump White House\n",
      "\n",
      "Word Mover's:\n",
      "Satire article: Secret Service Adds Emotional Protection Division To Safeguard Trump’s Psyche \n",
      "\n",
      "Candidate articles:\n",
      "Today in History: score: 1.86\n",
      "Amnesty: Up to ##,### Hanged in Syria's 'Slaughterhouse': score: 0.90\n",
      "Well, Then, Would You Like to Dance?: score: 0.88\n",
      "A Gravity-Defying Champion at Rest: score: 1.85\n",
      "Norman Rockwell's : score: 0.89\n"
     ]
    }
   ],
   "source": [
    "## Onion article with no related articles\n",
    "satire_article = all_combined['152378490']['title']\n",
    "candidate_articles = candidate_articles_list(all_combined['152378490'])\n",
    "print('tf-idf:')\n",
    "mapped_articles_tfidf('152378490',satire_article, candidate_articles)\n",
    "print(\"\\nWord Mover's:\")\n",
    "mapped_articles_wmd('152378490', satire_article, candidate_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf:\n",
      "Satire article: Intel Chiefs Say Trump's Twitter Account Was Hacked by Four-Year-Old  \n",
      "\n",
      "Candidate articles:\n",
      "New York Lawmakers Start the Year Weighted With Old Tensions\n",
      "Kanye West's Year of Breaking Bad\n",
      "Four Movies You Should Know About Before the Golden Globes\n",
      "A New Congress Is Sworn In, but With Many Old Faces\n",
      "The Latest: # House Democrats Say They'll Skip Inauguration\n",
      "\n",
      "Word Mover's:\n",
      "Satire article: Intel Chiefs Say Trump's Twitter Account Was Hacked by Four-Year-Old  \n",
      "\n",
      "Candidate articles:\n",
      "Tantrum on the No. #: score: 0.85\n",
      "Is Single-Sex Education Still Useful?: score: 1.79\n",
      "Word + Quiz: interregnum: score: 1.84\n",
      "Rake's Progress: A Look at the Well-Traveled Casanova: score: 1.83\n",
      "Jobs Report: What to Watch For: score: 1.84\n"
     ]
    }
   ],
   "source": [
    "## Borowitz Report article with no related articles\n",
    "satire_article = all_combined['145463847']['title']\n",
    "candidate_articles = candidate_articles_list(all_combined['145463847'])\n",
    "print('tf-idf:')\n",
    "mapped_articles_tfidf('145463847',satire_article, candidate_articles)\n",
    "print(\"\\nWord Mover's:\")\n",
    "mapped_articles_wmd('145463847', satire_article, candidate_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf:\n",
      "Satire article: Trump Economic Plan Calls for Every American to Inherit Millions from Father \n",
      "\n",
      "Candidate articles:\n",
      "Trump on Clinton: 'I Don't Think She's All There'\n",
      "Michael Phelps Wins His ##th Gold With American Relay Team\n",
      "When Every Company Is a Tech Company, Does the Label Matter?\n",
      "Starting With a Bang, an American Prodigy Wins Rio's First Gold\n",
      "Normandy Bar Fire Kills at Least ## at Birthday Party\n",
      "\n",
      "Word Mover's:\n",
      "Satire article: Trump Economic Plan Calls for Every American to Inherit Millions from Father \n",
      "\n",
      "Candidate articles:\n",
      "Golf Capsules: score: 1.85\n",
      "Ahmed H. Zewail, Nobel-Prize-Winning Chemist, Dies at ##: score: 1.85\n",
      "Donald Trump's Diet: He'll Have Fries With That: score: 1.85\n",
      "Alex Rodriguez's Orchestrated Move May Not Be His Last: score: 1.85\n",
      "What's on TV Sunday: 'Inspector Lewis' and Simone Biles at the Rio Olympics: score: 1.83\n"
     ]
    }
   ],
   "source": [
    "## Borowitz Report article with no related articles\n",
    "satire_article = all_combined['112329250']['title']\n",
    "candidate_articles = candidate_articles_list(all_combined['112329250'])\n",
    "print('tf-idf:')\n",
    "mapped_articles_tfidf('112329250',satire_article, candidate_articles)\n",
    "print(\"\\nWord Mover's:\")\n",
    "mapped_articles_wmd('112329250', satire_article, candidate_articles)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
